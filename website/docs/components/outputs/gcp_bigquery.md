---
title: gcp_bigquery
type: output
status: beta
categories: ["GCP","Services"]
---

<!--
     THIS FILE IS AUTOGENERATED!

     To make changes please edit the corresponding source file under internal/impl/<provider>.
-->

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

:::caution BETA
This component is mostly stable but breaking changes could still be made outside of major version releases if a fundamental problem with the component is found.
:::
Sends messages as new rows to a Google Cloud BigQuery table.

Introduced in version 3.55.0.


<Tabs defaultValue="common" values={[
  { label: 'Common', value: 'common', },
  { label: 'Advanced', value: 'advanced', },
]}>

<TabItem value="common">

```yml
# Common config fields, showing default values
output:
  label: ""
  gcp_bigquery:
    project: ""
    dataset: "" # No default (required)
    table: "" # No default (required)
    format: NEWLINE_DELIMITED_JSON
    max_in_flight: 64
    csv:
      header: []
      field_delimiter: ','
    batching:
      count: 0
      byte_size: 0
      period: ""
      check: ""
```

</TabItem>
<TabItem value="advanced">

```yml
# All config fields, showing default values
output:
  label: ""
  gcp_bigquery:
    project: ""
    dataset: "" # No default (required)
    table: "" # No default (required)
    format: NEWLINE_DELIMITED_JSON
    max_in_flight: 64
    csv:
      header: []
      field_delimiter: ','
      allow_jagged_rows: false
      allow_quoted_newlines: false
      encoding: UTF-8
      skip_leading_rows: 1
    batching:
      count: 0
      byte_size: 0
      period: ""
      check: ""
      processors: [] # No default (optional)
```

</TabItem>
</Tabs>

## Credentials

By default Benthos will use a shared credentials file when connecting to GCP services. You can find out more [in this document](/docs/guides/cloud/gcp).

## Format

This output currently supports only CSV and NEWLINE_DELIMITED_JSON formats. Learn more about how to use GCP BigQuery with them here:
- [`NEWLINE_DELIMITED_JSON`](https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-json)
- [`CSV`](https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv)

Each message may contain multiple elements separated by newlines. For example a single message containing:

```json
{"key": "1"}
{"key": "2"}
```

Is equivalent to two separate messages:

```json
{"key": "1"}
```

And:

```json
{"key": "2"}
```

The same is true for the CSV format.

### CSV

For the CSV format when the field `csv.header` is specified a header row will be inserted as the first line of each message batch. If this field is not provided then the first message of each message batch must include a header line.

## Performance

This output benefits from sending multiple messages in flight in parallel for
improved performance. You can tune the max number of in flight messages (or
message batches) with the field `max_in_flight`.

This output benefits from sending messages as a batch for improved performance.
Batches can be formed at both the input and output level. You can find out more
[in this doc](/docs/configuration/batching).

## Fields

### `project`

The project ID of the dataset to insert data to. If not set, it will be inferred from the credentials or read from the GOOGLE_CLOUD_PROJECT environment variable.


Type: `string`  
Default: `""`  

### `dataset`

The BigQuery Dataset ID.


Type: `string`  

### `table`

The table to insert messages to.


Type: `string`  

### `format`

The format of each incoming message.


Type: `string`  
Default: `"NEWLINE_DELIMITED_JSON"`  
Options: `NEWLINE_DELIMITED_JSON`, `CSV`.

### `max_in_flight`

The maximum number of message batches to have in flight at a given time. Increase this to improve throughput.


Type: `int`  
Default: `64`  

### `csv`

Specify how CSV data should be interpretted.


Type: `object`  

### `csv.header`

A list of values to use as header for each batch of messages. If not specified the first line of each message will be used as header.


Type: `array`  
Default: `[]`  

### `csv.field_delimiter`

The separator for fields in a CSV file, used when reading or exporting data.


Type: `string`  
Default: `","`  

### `csv.allow_jagged_rows`

Causes missing trailing optional columns to be tolerated when reading CSV data. Missing values are treated as nulls.


Type: `bool`  
Default: `false`  

### `csv.allow_quoted_newlines`

Sets whether quoted data sections containing newlines are allowed when reading CSV data.


Type: `bool`  
Default: `false`  

### `csv.encoding`

Encoding is the character encoding of data to be read.


Type: `string`  
Default: `"UTF-8"`  
Options: `UTF-8`, `ISO-8859-1`.

### `csv.skip_leading_rows`

The number of rows at the top of a CSV file that BigQuery will skip when reading data. The default value is 1 since Benthos will add the specified header in the first line of each batch sent to BigQuery.


Type: `int`  
Default: `1`  

### `batching`

Allows you to configure a [batching policy](/docs/configuration/batching).


Type: `object`  

```yml
# Examples

batching:
  byte_size: 5000
  count: 0
  period: 1s

batching:
  count: 10
  period: 1s

batching:
  check: this.contains("END BATCH")
  count: 0
  period: 1m
```

### `batching.count`

A number of messages at which the batch should be flushed. If `0` disables count based batching.


Type: `int`  
Default: `0`  

### `batching.byte_size`

An amount of bytes at which the batch should be flushed. If `0` disables size based batching.


Type: `int`  
Default: `0`  

### `batching.period`

A period in which an incomplete batch should be flushed regardless of its size.


Type: `string`  
Default: `""`  

```yml
# Examples

period: 1s

period: 1m

period: 500ms
```

### `batching.check`

A [Bloblang query](/docs/guides/bloblang/about/) that should return a boolean value indicating whether a message should end a batch.


Type: `string`  
Default: `""`  

```yml
# Examples

check: this.type == "end_of_transaction"
```

### `batching.processors`

A list of [processors](/docs/components/processors/about) to apply to a batch as it is flushed. This allows you to aggregate and archive the batch however you see fit. Please note that all resulting messages are flushed as a single batch, therefore splitting the batch into smaller batches using these processors is a no-op.


Type: `array`  

```yml
# Examples

processors:
  - archive:
      format: concatenate

processors:
  - archive:
      format: lines

processors:
  - archive:
      format: json_array
```


