= oracledb_cdc
:type: input
:status: beta
:categories: ["Services"]



////
     THIS FILE IS AUTOGENERATED!

     To make changes, edit the corresponding source file under:

     https://github.com/redpanda-data/connect/tree/main/internal/impl/<provider>.

     And:

     https://github.com/redpanda-data/connect/tree/main/cmd/tools/docs_gen/templates/plugin.adoc.tmpl
////

// Â© 2024 Redpanda Data Inc.


component_type_dropdown::[]


Enables Change Data Capture by consuming from OracleDB.

Introduced in version 0.0.1.


[tabs]
======
Common::
+
--

```yml
# Common config fields, showing default values
input:
  label: ""
  oracledb_cdc:
    connection_string: oracle://username:password@host:port/service_name # No default (required)
    stream_snapshot: false
    max_parallel_snapshot_tables: 1
    snapshot_max_batch_size: 1000
    logminer:
      max_batch_size: 500
      backoff_interval: 5s
      strategy: online_catalog
    include: [] # No default (required)
    exclude: [] # No default (optional)
    checkpoint_cache: "" # No default (optional)
    checkpoint_cache_table_name: RPCN.CDC_CHECKPOINT_CACHE
    checkpoint_cache_key: oracledb_cdc
    checkpoint_limit: 1024
    auto_replay_nacks: true
    batching:
      count: 0
      byte_size: 0
      period: ""
      check: ""
```

--
Advanced::
+
--

```yml
# All config fields, showing default values
input:
  label: ""
  oracledb_cdc:
    connection_string: oracle://username:password@host:port/service_name # No default (required)
    stream_snapshot: false
    max_parallel_snapshot_tables: 1
    snapshot_max_batch_size: 1000
    logminer:
      max_batch_size: 500
      backoff_interval: 5s
      strategy: online_catalog
    include: [] # No default (required)
    exclude: [] # No default (optional)
    checkpoint_cache: "" # No default (optional)
    checkpoint_cache_table_name: RPCN.CDC_CHECKPOINT_CACHE
    checkpoint_cache_key: oracledb_cdc
    checkpoint_limit: 1024
    auto_replay_nacks: true
    batching:
      count: 0
      byte_size: 0
      period: ""
      check: ""
      processors: [] # No default (optional)
```

--
======

Streams changes from an Oracle database for Change Data Capture (CDC).
Additionally, if `stream_snapshot` is set to true, then the existing data in the database is also streamed too.

== Metadata

This input adds the following metadata fields to each message:
- schema (Schema of the table that the message originated from)
- table (Name of the table that the message originated from)
- operation (Type of operation that generated the message: "read", "delete", "insert", or "update_before" and "update_after". "read" is from messages that are read in the initial snapshot phase.)
- scn (the System Change Number in Oracle)

== Permissions

When using the default Oracle based cache, the Connect user requires permission to create tables and stored procedures, and the rpcn  schema must already exist. Refer to `checkpoint_cache_table_name` for more information.
		

== Fields

=== `connection_string`

The connection string of the Oracle database to connect to.


*Type*: `string`


```yml
# Examples

connection_string: oracle://username:password@host:port/service_name
```

=== `stream_snapshot`

If set to true, the connector will query all the existing data as a part of snapshot process. Otherwise, it will start from the current System Change Number position.


*Type*: `bool`

*Default*: `false`

```yml
# Examples

stream_snapshot: true
```

=== `max_parallel_snapshot_tables`

Specifies a number of tables that will be processed in parallel during the snapshot processing stage.


*Type*: `int`

*Default*: `1`

=== `snapshot_max_batch_size`

The maximum number of rows to be streamed in a single batch when taking a snapshot.


*Type*: `int`

*Default*: `1000`

=== `logminer`

LogMiner configuration settings.


*Type*: `object`


=== `logminer.max_batch_size`

The maximum number of records to be queried when parsing log lines via LogMiner. Smaller batches mean more frequent queries with higher overhead but lower latency, larger batches mean fewer queries with better throughput but require more memory.


*Type*: `int`

*Default*: `500`

=== `logminer.backoff_interval`

The interval between attempts to check for new changes once all data is processed. For low traffic tables increasing this value can reduce network traffic to the server.


*Type*: `string`

*Default*: `"5s"`

```yml
# Examples

backoff_interval: 5s

backoff_interval: 1m
```

=== `logminer.strategy`

Controls how LogMiner retrieves data dictionary information. `online_catalog` (default) uses the current data dictionary for best performance but cannot capture DDL changes. `online_catalog` currently only supported.


*Type*: `string`

*Default*: `"online_catalog"`

=== `include`

Regular expressions for tables to include.


*Type*: `array`


```yml
# Examples

include: SCHEMA.PRODUCTS
```

=== `exclude`

Regular expressions for tables to exclude.


*Type*: `array`


```yml
# Examples

exclude: SCHEMA.PRIVATETABLE
```

=== `checkpoint_cache`

A https://www.docs.redpanda.com/redpanda-connect/components/caches/about[cache resource^] to use for storing the current System Change Number (SCN) that has been successfully delivered, this allows Redpanda Connect to continue from that System Change Number (SCN) upon restart, rather than consume the entire state of OracleDB's redo logs. If not set the default Oracle based cache will be used, see `checkpoint_cache_table_name` for more information.


*Type*: `string`


=== `checkpoint_cache_table_name`

The identifier for the checkpoint cache table name. If no `checkpoint_cache` field is specified, this input will automatically create a table and stored procedure under the `rpcn` schema to act as a checkpoint cache. This table stores the latest processed System Change Number (SCN) that has been successfully delivered, allowing Redpanda Connect to resume from that point upon restart rather than reconsume the entire redo log.


*Type*: `string`

*Default*: `"RPCN.CDC_CHECKPOINT_CACHE"`

```yml
# Examples

checkpoint_cache_table_name: RPCN.CHECKPOINT_CACHE
```

=== `checkpoint_cache_key`

The key to use to store the snapshot position in `checkpoint_cache`. An alternative key can be provided if multiple CDC inputs share the same cache.


*Type*: `string`

*Default*: `"oracledb_cdc"`

=== `checkpoint_limit`

The maximum number of messages that can be processed at a given time. Increasing this limit enables parallel processing and batching at the output level. Any given System Change Number (SCN) will not be acknowledged unless all messages under that offset are delivered in order to preserve at least once delivery guarantees.


*Type*: `int`

*Default*: `1024`

=== `auto_replay_nacks`

Whether messages that are rejected (nacked) at the output level should be automatically replayed indefinitely, eventually resulting in back pressure if the cause of the rejections is persistent. If set to `false` these messages will instead be deleted. Disabling auto replays can greatly improve memory efficiency of high throughput streams as the original shape of the data can be discarded immediately upon consumption and mutation.


*Type*: `bool`

*Default*: `true`

=== `batching`

Allows you to configure a xref:configuration:batching.adoc[batching policy].


*Type*: `object`


```yml
# Examples

batching:
  byte_size: 5000
  count: 0
  period: 1s

batching:
  count: 10
  period: 1s

batching:
  check: this.contains("END BATCH")
  count: 0
  period: 1m
```

=== `batching.count`

A number of messages at which the batch should be flushed. If `0` disables count based batching.


*Type*: `int`

*Default*: `0`

=== `batching.byte_size`

An amount of bytes at which the batch should be flushed. If `0` disables size based batching.


*Type*: `int`

*Default*: `0`

=== `batching.period`

A period in which an incomplete batch should be flushed regardless of its size.


*Type*: `string`

*Default*: `""`

```yml
# Examples

period: 1s

period: 1m

period: 500ms
```

=== `batching.check`

A xref:guides:bloblang/about.adoc[Bloblang query] that should return a boolean value indicating whether a message should end a batch.


*Type*: `string`

*Default*: `""`

```yml
# Examples

check: this.type == "end_of_transaction"
```

=== `batching.processors`

A list of xref:components:processors/about.adoc[processors] to apply to a batch as it is flushed. This allows you to aggregate and archive the batch however you see fit. Please note that all resulting messages are flushed as a single batch, therefore splitting the batch into smaller batches using these processors is a no-op.


*Type*: `array`


```yml
# Examples

processors:
  - archive:
      format: concatenate

processors:
  - archive:
      format: lines

processors:
  - archive:
      format: json_array
```


