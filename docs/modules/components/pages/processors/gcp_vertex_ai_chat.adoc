= gcp_vertex_ai_chat
:type: processor
:status: experimental
:categories: ["AI"]



////
     THIS FILE IS AUTOGENERATED!

     To make changes, edit the corresponding source file under:

     https://github.com/redpanda-data/connect/tree/main/internal/impl/<provider>.

     And:

     https://github.com/redpanda-data/connect/tree/main/cmd/tools/docs_gen/templates/plugin.adoc.tmpl
////

// Â© 2024 Redpanda Data Inc.


component_type_dropdown::[]


Generates responses to messages in a chat conversation, using the Vertex AI API.

Introduced in version 4.33.0.


[tabs]
======
Common::
+
--

```yml
# Common config fields, showing default values
label: ""
gcp_vertex_ai_chat:
  project: "" # No default (required)
  credentials_json: "" # No default (optional)
  location: us-central1 # No default (optional)
  model: gemini-1.5-pro-001 # No default (required)
  prompt: "" # No default (optional)
  temperature: 0 # No default (optional)
  max_tokens: 0 # No default (optional)
  response_format: text
```

--
Advanced::
+
--

```yml
# All config fields, showing default values
label: ""
gcp_vertex_ai_chat:
  project: "" # No default (required)
  credentials_json: "" # No default (optional)
  location: us-central1 # No default (optional)
  model: gemini-1.5-pro-001 # No default (required)
  prompt: "" # No default (optional)
  system_prompt: "" # No default (optional)
  temperature: 0 # No default (optional)
  max_tokens: 0 # No default (optional)
  response_format: text
  top_p: 0 # No default (optional)
  top_k: 0 # No default (optional)
  stop: [] # No default (optional)
  presence_penalty: 0 # No default (optional)
  frequency_penalty: 0 # No default (optional)
```

--
======

This processor sends prompts to your chosen large language model (LLM) and generates text from the responses, using the Vertex AI API.

For more information, see the https://cloud.google.com/vertex-ai/docs[Vertex AI documentation^].

== Fields

=== `project`

GCP project ID to use


*Type*: `string`


=== `credentials_json`

An optional field to set google Service Account Credentials json.
[CAUTION]
====
This field contains sensitive information that usually shouldn't be added to a config directly, read our xref:configuration:secrets.adoc[secrets page for more info].
====



*Type*: `string`


=== `location`

The location of the model if using a fined tune model. For base models this can be omitted


*Type*: `string`


```yml
# Examples

location: us-central1
```

=== `model`

The name of the LLM to use. For a full list of models, see the https://console.cloud.google.com/vertex-ai/model-garden[Vertex AI Model Garden].


*Type*: `string`


```yml
# Examples

model: gemini-1.5-pro-001

model: gemini-1.5-flash-001
```

=== `prompt`

The prompt you want to generate a response for. By default, the processor submits the entire payload as a string.
This field supports xref:configuration:interpolation.adoc#bloblang-queries[interpolation functions].


*Type*: `string`


=== `system_prompt`

The system prompt to submit to the Vertex AI LLM.
This field supports xref:configuration:interpolation.adoc#bloblang-queries[interpolation functions].


*Type*: `string`


=== `temperature`

Controls the randomness of predications.


*Type*: `float`


=== `max_tokens`

The maximum number of output tokens to generate per message.


*Type*: `int`


=== `response_format`

The response format of generated type, the model must also be prompted to output the appropriate response type.


*Type*: `string`

*Default*: `"text"`

Options:
`text`
, `json`
.

=== `top_p`

If specified, nucleus sampling will be used.


*Type*: `float`


=== `top_k`

If specified top-k sampling will be used.


*Type*: `int`


=== `stop`

Stop sequences to when the model will stop generating further tokens.


*Type*: `array`


=== `presence_penalty`

Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.


*Type*: `float`


=== `frequency_penalty`

Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.


*Type*: `float`



