= gcp_vertex_ai_chat
:type: processor
:status: experimental
:categories: ["AI"]



////
     THIS FILE IS AUTOGENERATED!

     To make changes, edit the corresponding source file under:

     https://github.com/redpanda-data/connect/tree/main/internal/impl/<provider>.

     And:

     https://github.com/redpanda-data/connect/tree/main/cmd/tools/docs_gen/templates/plugin.adoc.tmpl
////

// Â© 2024 Redpanda Data Inc.


component_type_dropdown::[]


Generates responses to messages in a chat conversation, using the Vertex AI API.

Introduced in version 4.34.0.


[tabs]
======
Common::
+
--

```yml
# Common config fields, showing default values
label: ""
gcp_vertex_ai_chat:
  project: "" # No default (required)
  credentials_json: "" # No default (optional)
  location: us-central1 # No default (required)
  model: gemini-1.5-pro-001 # No default (required)
  prompt: "" # No default (optional)
  history: "" # No default (optional)
  attachment: 'root = this.image.decode("base64") # decode base64 encoded image' # No default (optional)
  temperature: 0 # No default (optional)
  max_tokens: 0 # No default (optional)
  response_format: text
  tools: []
```

--
Advanced::
+
--

```yml
# All config fields, showing default values
label: ""
gcp_vertex_ai_chat:
  project: "" # No default (required)
  credentials_json: "" # No default (optional)
  location: us-central1 # No default (required)
  model: gemini-1.5-pro-001 # No default (required)
  prompt: "" # No default (optional)
  system_prompt: "" # No default (optional)
  history: "" # No default (optional)
  attachment: 'root = this.image.decode("base64") # decode base64 encoded image' # No default (optional)
  temperature: 0 # No default (optional)
  max_tokens: 0 # No default (optional)
  response_format: text
  top_p: 0 # No default (optional)
  top_k: 0 # No default (optional)
  stop: [] # No default (optional)
  presence_penalty: 0 # No default (optional)
  frequency_penalty: 0 # No default (optional)
  max_tool_calls: 10
  tools: []
```

--
======

This processor sends prompts to your chosen large language model (LLM) and generates text from the responses, using the Vertex AI API.

For more information, see the https://cloud.google.com/vertex-ai/docs[Vertex AI documentation^].

== Examples

[tabs]
======
Use processors as tool calls::
+
--

This example allows gemini to execute a subpipeline as a tool call to get more data.

```yaml
input:
  generate:
    count: 1
    mapping: |
      root = "What is the weather like in Chicago?"
pipeline:
  processors:
    - gcp_vertex_ai_chat:
        model: gemini-2.5-flash-preview-05-20
        project: my-project
        location: us-central1
        prompt: "${!content().string()}"
        tools:
          - name: GetWeather
            description: "Retrieve the weather for a specific city"
            parameters:
              required: ["city"]
              properties:
                city:
                  type: string
                  description: the city to lookup the weather for
            processors:
              - http:
                  verb: GET
                  url: 'https://wttr.in/${!this.city}?T'
                  headers:
                    # Spoof curl user-agent to get a plaintext text
                    User-Agent: curl/8.11.1
output:
  stdout: {}
```

--
======

== Fields

=== `project`

GCP project ID to use


*Type*: `string`


=== `credentials_json`

An optional field to set google Service Account Credentials json.
[CAUTION]
====
This field contains sensitive information that usually shouldn't be added to a config directly, read our xref:configuration:secrets.adoc[secrets page for more info].
====



*Type*: `string`


=== `location`

The location of the model if using a fined tune model. For base models this can be omitted


*Type*: `string`


```yml
# Examples

location: us-central1
```

=== `model`

The name of the LLM to use. For a full list of models, see the https://console.cloud.google.com/vertex-ai/model-garden[Vertex AI Model Garden].


*Type*: `string`


```yml
# Examples

model: gemini-1.5-pro-001

model: gemini-1.5-flash-001
```

=== `prompt`

The prompt you want to generate a response for. By default, the processor submits the entire payload as a string.
This field supports xref:configuration:interpolation.adoc#bloblang-queries[interpolation functions].


*Type*: `string`


=== `system_prompt`

The system prompt to submit to the Vertex AI LLM.
This field supports xref:configuration:interpolation.adoc#bloblang-queries[interpolation functions].


*Type*: `string`


=== `history`

Historical messages to include in the chat request. The result of the bloblang query should be an array of objects of the form of [{"role": "", "content":""}], where role is "user" or "model".


*Type*: `string`


=== `attachment`

Additional data like an image to send with the prompt to the model. The result of the mapping must be a byte array, and the content type is automatically detected.


*Type*: `string`

Requires version 4.38.0 or newer

```yml
# Examples

attachment: 'root = this.image.decode("base64") # decode base64 encoded image'
```

=== `temperature`

Controls the randomness of predications.


*Type*: `float`


=== `max_tokens`

The maximum number of output tokens to generate per message.


*Type*: `int`


=== `response_format`

The response format of generated type, the model must also be prompted to output the appropriate response type.


*Type*: `string`

*Default*: `"text"`

Options:
`text`
, `json`
.

=== `top_p`

If specified, nucleus sampling will be used.


*Type*: `float`


=== `top_k`

If specified top-k sampling will be used.


*Type*: `float`


=== `stop`

Stop sequences to when the model will stop generating further tokens.


*Type*: `array`


=== `presence_penalty`

Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.


*Type*: `float`


=== `frequency_penalty`

Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.


*Type*: `float`


=== `max_tool_calls`

The maximum number of sequential tool calls.


*Type*: `int`

*Default*: `10`

=== `tools`

The tools to allow the LLM to invoke. This allows building subpipelines that the LLM can choose to invoke to execute agentic-like actions.


*Type*: `array`

*Default*: `[]`

=== `tools[].name`

The name of this tool.


*Type*: `string`


=== `tools[].description`

A description of this tool, the LLM uses this to decide if the tool should be used.


*Type*: `string`


=== `tools[].parameters`

The parameters the LLM needs to provide to invoke this tool.


*Type*: `object`


=== `tools[].parameters.required`

The required parameters for this pipeline.


*Type*: `array`

*Default*: `[]`

=== `tools[].parameters.properties`

The properties for the processor's input data


*Type*: `object`


=== `tools[].parameters.properties.<name>.type`

The type of this parameter.


*Type*: `string`


=== `tools[].parameters.properties.<name>.description`

A description of this parameter.


*Type*: `string`


=== `tools[].parameters.properties.<name>.enum`

Specifies that this parameter is an enum and only these specific values should be used.


*Type*: `array`

*Default*: `[]`

=== `tools[].processors`

The pipeline to execute when the LLM uses this tool.


*Type*: `array`



