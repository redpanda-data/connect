= ollama_chat
:type: processor
:status: experimental
:categories: ["AI"]



////
     THIS FILE IS AUTOGENERATED!

     To make changes, edit the corresponding source file under:

     https://github.com/redpanda-data/connect/tree/main/internal/impl/<provider>.

     And:

     https://github.com/redpanda-data/connect/tree/main/cmd/tools/docs_gen/templates/plugin.adoc.tmpl
////

// Â© 2024 Redpanda Data Inc.


component_type_dropdown::[]


Processor that uses the Ollama API to generate text.

Introduced in version 4.32.0.


[tabs]
======
Common::
+
--

```yml
# Common config fields, showing default values
label: ""
ollama_chat:
  server_address: http://127.0.0.1:11434 # No default (optional)
  model: llama3 # No default (required)
  prompt: "" # No default (optional)
```

--
Advanced::
+
--

```yml
# All config fields, showing default values
label: ""
ollama_chat:
  server_address: http://127.0.0.1:11434 # No default (optional)
  model: llama3 # No default (required)
  prompt: "" # No default (optional)
  system_prompt: "" # No default (optional)
```

--
======

This processor sends prompts to your chosen Ollama large language model (LLM) and generates text from the responses, using the Ollama API.

By default this processor will start a locally installed Ollama server. Ollama can be installed by following the instructions found https://ollama.com/download[here^]. An already running Ollama server can be used by configuring `server_address`.

For more information, see the https://ollama.com/[Ollama website^].

== Fields

=== `server_address`

The address of the Ollama server to use. By default, a local Ollama server starts and runs unless you specify the address of a local or remote server.


*Type*: `string`


```yml
# Examples

server_address: http://127.0.0.1:11434
```

=== `model`

The name of the Ollama LLM to use. For a full list of models, see the https://ollama.com/models[Ollama website].


*Type*: `string`


```yml
# Examples

model: llama3

model: gemma2

model: qwen2

model: phi3
```

=== `prompt`

The prompt you want to generate a response for. By default, the processor submits the entire payload as a string.
This field supports xref:configuration:interpolation.adoc#bloblang-queries[interpolation functions].


*Type*: `string`


=== `system_prompt`

The system prompt to submit to the Ollama LLM.
This field supports xref:configuration:interpolation.adoc#bloblang-queries[interpolation functions].


*Type*: `string`



