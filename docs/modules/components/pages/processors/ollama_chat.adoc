= ollama_chat
:type: processor
:status: experimental
:categories: ["AI"]



////
     THIS FILE IS AUTOGENERATED!

     To make changes, edit the corresponding source file under:

     https://github.com/redpanda-data/connect/tree/main/internal/impl/<provider>.

     And:

     https://github.com/redpanda-data/connect/tree/main/cmd/tools/docs_gen/templates/plugin.adoc.tmpl
////

// Â© 2024 Redpanda Data Inc.


component_type_dropdown::[]


Generates responses to messages in a chat conversation, using the Ollama API.

Introduced in version 4.32.0.


[tabs]
======
Common::
+
--

```yml
# Common config fields, showing default values
label: ""
ollama_chat:
  server_address: http://127.0.0.1:11434 # No default (optional)
  model: llama3 # No default (required)
  prompt: "" # No default (optional)
```

--
Advanced::
+
--

```yml
# All config fields, showing default values
label: ""
ollama_chat:
  server_address: http://127.0.0.1:11434 # No default (optional)
  model: llama3 # No default (required)
  prompt: "" # No default (optional)
  system_prompt: "" # No default (optional)
```

--
======

This processor sends prompts to your chosen Ollama large language model (LLM) and generates text from the responses, using the Ollama API.

By default, the processor starts and runs a locally installed Ollama server. Alternatively, to use an already running Ollama server, add your server details to the `server_address` field. You can https://ollama.com/download[download and install Ollama from the Ollama website^].

For more information, see the https://github.com/ollama/ollama/tree/main/docs[Ollama documentation^].

== Fields

=== `server_address`

The address of the Ollama server to use. Leave the field blank and the processor starts and runs a local Ollama server or specify the address of your own local or remote server.


*Type*: `string`


```yml
# Examples

server_address: http://127.0.0.1:11434
```

=== `model`

The name of the Ollama LLM to use. For a full list of models, see the https://ollama.com/models[Ollama website].


*Type*: `string`


```yml
# Examples

model: llama3

model: gemma2

model: qwen2

model: phi3
```

=== `prompt`

The prompt you want to generate a response for. By default, the processor submits the entire payload as a string.
This field supports xref:configuration:interpolation.adoc#bloblang-queries[interpolation functions].


*Type*: `string`


=== `system_prompt`

The system prompt to submit to the Ollama LLM.
This field supports xref:configuration:interpolation.adoc#bloblang-queries[interpolation functions].


*Type*: `string`



