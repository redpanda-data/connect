= parquet_decode
:type: processor
:status: experimental
:categories: ["Parsing"]



////
     THIS FILE IS AUTOGENERATED!

     To make changes, edit the corresponding source file under:

     https://github.com/redpanda-data/connect/tree/main/internal/impl/<provider>.

     And:

     https://github.com/redpanda-data/connect/tree/main/cmd/tools/docs_gen/templates/plugin.adoc.tmpl
////

// Â© 2024 Redpanda Data Inc.


component_type_dropdown::[]


Decodes https://parquet.apache.org/docs/[Parquet files^] into a batch of structured messages.

Introduced in version 4.4.0.

```yml
# Config fields, showing default values
label: ""
parquet_decode:
  handle_logical_types: 0
```

This processor uses https://github.com/parquet-go/parquet-go[https://github.com/parquet-go/parquet-go^], which is itself experimental. Therefore changes could be made into how this processor functions outside of major version releases.

== Fields

=== `handle_logical_types`

Whether to be smart about decoding logical types. In the Parquet format, logical types are stored as one of the standard physical types with some additional metadata describing the logical type. For example, UUIDs are stored in a FIXED_LEN_BYTE_ARRAY physical type, but there is metadata in the schema denoting that it is a UUID. By default, this logical type metadata will be ignored and values will be decoded directly from the physical type, which isn't always desirable. By enabling this option, logical types will be given special treatment and will decode into more useful values.

The following list describes how logical types are handled based on the value of this field:
- `handle_logical_types >= 1`
  - TIMESTAMP - decodes as an RFC3339 string describing the time. If `isAdjustedToUTC` is set to true the time zone will be set to UTC, if it is set to false the time zone will be set to local time.
  - UUID - decodes as a string, i.e. `00112233-4455-6677-8899-aabbccddeeff`.


*Type*: `int`

*Default*: `0`

== Examples

[tabs]
======
Reading Parquet Files from AWS S3::
+
--

In this example we consume files from AWS S3 as they're written by listening onto an SQS queue for upload events. We make sure to use the `to_the_end` scanner which means files are read into memory in full, which then allows us to use a `parquet_decode` processor to expand each file into a batch of messages. Finally, we write the data out to local files as newline delimited JSON.

```yaml
input:
  aws_s3:
    bucket: TODO
    prefix: foos/
    scanner:
      to_the_end: {}
    sqs:
      url: TODO
  processors:
    - parquet_decode: {}

output:
  file:
    codec: lines
    path: './foos/${! meta("s3_key") }.jsonl'
```

--
======


